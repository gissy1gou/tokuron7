{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68点いった！\n",
    "Data AugmentationとFineTuningしよう！\n",
    "わかりやすくなるようにいらんやつけそう\n",
    "\n",
    "DataAugmentation 完了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 犬と猫を分類するモデルを構築\n",
    "\n",
    "### 1. 画像の読み込み\n",
    "\n",
    "まずは簡単に画像の読み込みの方法を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/takekazukitagishi/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow',\n",
      " '/Applications/anaconda3/lib/python37.zip',\n",
      " '/Applications/anaconda3/lib/python3.7',\n",
      " '/Applications/anaconda3/lib/python3.7/lib-dynload',\n",
      " '',\n",
      " '/Applications/anaconda3/lib/python3.7/site-packages',\n",
      " '/Applications/anaconda3/lib/python3.7/site-packages/aeosa',\n",
      " '/Applications/anaconda3/lib/python3.7/site-packages/IPython/extensions',\n",
      " '/Users/takekazukitagishi/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pprint\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1,'/Users/takekazukitagishi/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/takekazukitagishi/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow',\n",
      " '/Users/takekazukitagishi/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages',\n",
      " '/Applications/anaconda3/lib/python37.zip',\n",
      " '/Applications/anaconda3/lib/python3.7',\n",
      " '/Applications/anaconda3/lib/python3.7/lib-dynload',\n",
      " '',\n",
      " '/Applications/anaconda3/lib/python3.7/site-packages',\n",
      " '/Applications/anaconda3/lib/python3.7/site-packages/aeosa',\n",
      " '/Applications/anaconda3/lib/python3.7/site-packages/IPython/extensions',\n",
      " '/Users/takekazukitagishi/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pprint\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "追加終わり"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データオーギュメンテーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport PIL\\nfrom keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator,array_to_img\\nimport os\\ndog_filepaths = glob('train/dog/*.jpg')\\ncat_filepaths = glob('train/cat/*.jpg')\\nx, t = [], []\\n# ImageDataGeneratorをつくる\\n# datagenは「45度の範囲でランダムに回転するやつ」など\\ndatagen = ImageDataGenerator(rotation_range=45)\\ndatagen2 = ImageDataGenerator(shear_range=0.85)\\ndatagen3 = ImageDataGenerator(horizontal_flip=0.3)\\ndatagen4 = ImageDataGenerator(vertical_flip=0.3)\\n#これだとデータのパス（dog_filepaths）中の各ファイルに対して、imgファイルを開いてnumpyに変換してx,tの配列にぶちこんでる\\nfor filepath in dog_filepaths:\\n    img  = Image.open(filepath)\\n    img = np.array(img)\\n    #img.shapeすると(224,224,3)\\n    x.append(img)\\n    t.append(np.array(0))\\n    #ImageDataGeneratorは4次元データじゃないと読み込まないっぽい\\n    img = img.reshape((1,) + img.shape)\\n     #img.shapeすると(1,224,224,3)\\n    g = datagen.flow(img, batch_size=1)\\n    for i in range(3):\\n        batches = g.next()\\n         #batchesは(1,224,224,3)になっているから、batchesの[0]番目の配列を指定してあげる\\n        x.append(batches[0])\\n        t.append(np.array(0))\\n    #面倒くさいからg2,g3,g4も同時にぶちこんでる\\n    g2 = datagen2.flow(img, batch_size=1)\\n    for i in range(3):\\n        batches = g2.next()\\n        x.append(batches[0])\\n        t.append(np.array(0))\\n    g3 = datagen3.flow(img, batch_size=1)\\n    for i in range(3):\\n        batches = g3.next()\\n        x.append(batches[0])\\n        t.append(np.array(0))\\n    g4 = datagen4.flow(img, batch_size=1)\\n    for i in range(3):\\n        batches = g4.next()\\n        x.append(batches[0])\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import PIL\n",
    "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator,array_to_img\n",
    "import os\n",
    "dog_filepaths = glob('train/dog/*.jpg')\n",
    "cat_filepaths = glob('train/cat/*.jpg')\n",
    "x, t = [], []\n",
    "# ImageDataGeneratorをつくる\n",
    "# datagenは「45度の範囲でランダムに回転するやつ」など\n",
    "datagen = ImageDataGenerator(rotation_range=45)\n",
    "datagen2 = ImageDataGenerator(shear_range=0.85)\n",
    "datagen3 = ImageDataGenerator(horizontal_flip=0.3)\n",
    "datagen4 = ImageDataGenerator(vertical_flip=0.3)\n",
    "#これだとデータのパス（dog_filepaths）中の各ファイルに対して、imgファイルを開いてnumpyに変換してx,tの配列にぶちこんでる\n",
    "for filepath in dog_filepaths:\n",
    "    img  = Image.open(filepath)\n",
    "    img = np.array(img)\n",
    "    #img.shapeすると(224,224,3)\n",
    "    x.append(img)\n",
    "    t.append(np.array(0))\n",
    "    #ImageDataGeneratorは4次元データじゃないと読み込まないっぽい\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "     #img.shapeすると(1,224,224,3)\n",
    "    g = datagen.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g.next()\n",
    "         #batchesは(1,224,224,3)になっているから、batchesの[0]番目の配列を指定してあげる\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))\n",
    "    #面倒くさいからg2,g3,g4も同時にぶちこんでる\n",
    "    g2 = datagen2.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g2.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))\n",
    "    g3 = datagen3.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g3.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))\n",
    "    g4 = datagen4.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g4.next()\n",
    "        x.append(batches[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "終わり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` のフォルダに入っている画像を読み込みます。\n",
    "`glob` を利用すると簡単にファイルを検索できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_filepaths = glob('train/dog/*.jpg')\n",
    "cat_filepaths = glob('train/cat/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "# pip install Pillow    or   python -m pip install Pillow\n",
    "\n",
    "# macOS\n",
    "# pip3 install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. クラスラベルの割り振り\n",
    "\n",
    "画像の読み込み方がわかったため、クラスのラベルを割り振っていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator,array_to_img\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力値:x, 目標値: t\n",
    "x, t = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGeneratorをつくる\n",
    "# datagenは「45度の範囲でランダムに回転するやつ」など\n",
    "datagen = ImageDataGenerator(rotation_range=45)\n",
    "datagen2 = ImageDataGenerator(shear_range=0.85)\n",
    "datagen3 = ImageDataGenerator(horizontal_flip=0.3)\n",
    "datagen4 = ImageDataGenerator(vertical_flip=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 犬\n",
    "for filepath in dog_filepaths:\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize((224, 224))#追加\n",
    "    img = np.array(img)\n",
    "    x.append(img)\n",
    "    t.append(np.array(0))  # 犬は 0 とする\n",
    "    \n",
    "    ###\n",
    "    \"\"\"\n",
    "    # ImageDataGeneratorをつくる\n",
    "    # datagenは「45度の範囲でランダムに回転するやつ」など\n",
    "    datagen = ImageDataGenerator(rotation_range=45)\n",
    "    datagen2 = ImageDataGenerator(shear_range=0.85)\n",
    "    datagen3 = ImageDataGenerator(horizontal_flip=0.3)\n",
    "    datagen4 = ImageDataGenerator(vertical_flip=0.3)\n",
    "    \"\"\"\n",
    "    #ImageDataGeneratorは4次元データじゃないと読み込まないっぽい\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "     #img.shapeすると(1,224,224,3)\n",
    "    g = datagen.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g.next()\n",
    "         #batchesは(1,224,224,3)になっているから、batchesの[0]番目の配列を指定してあげる\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))\n",
    "    #面倒くさいからg2,g3,g4も同時にぶちこんでる\n",
    "    g2 = datagen2.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g2.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))\n",
    "    g3 = datagen3.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g3.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))\n",
    "    g4 = datagen4.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g4.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 猫\n",
    "for filepath in cat_filepaths:\n",
    "    img = Image.open(filepath)\n",
    "    img = img.resize((224, 224))#追加\n",
    "    img = np.array(img)\n",
    "    x.append(img)\n",
    "    t.append(np.array(1))  # 猫は 1 とする\n",
    "    \n",
    "    ##\n",
    "        \n",
    "     #ImageDataGeneratorは4次元データじゃないと読み込まないっぽい\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "     #img.shapeすると(1,224,224,3)\n",
    "    g = datagen.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g.next()\n",
    "         #batchesは(1,224,224,3)になっているから、batchesの[0]番目の配列を指定してあげる\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(1))\n",
    "    #面倒くさいからg2,g3,g4も同時にぶちこんでる\n",
    "    g2 = datagen2.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g2.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(1))\n",
    "    g3 = datagen3.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g3.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(1))\n",
    "    g4 = datagen4.flow(img, batch_size=1)\n",
    "    for i in range(3):\n",
    "        batches = g4.next()\n",
    "        x.append(batches[0])\n",
    "        t.append(np.array(1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 3900)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 格納された枚数\n",
    "len(x), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全体を numpy の形式に変換\n",
    "x = np.array(x)  # f は float32\n",
    "t = np.array(t)  # i は int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 224, 224, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 300(枚), 224(height), 224(width), 3(channels)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x の値の範囲を正規化しておくと学習が効率的に進むことが多いため、こちらも行なっておきましょう。学習係数などを調整する場合など、必ず必要ではありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の正規化 (0~1の範囲に)\n",
    "x = x / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 訓練データと検証データに分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow では訓練データ (train) と検証データ (val) を分ける機能がないため、scikit-learn の `train_test_split` を利用します。\n",
    "\n",
    "なお、テストデータ (test) は私の手元にしかないため、手持ちのデータの中で以下にテストデータを模擬的に表現できるかのために検証データがあります。精度を高めたいときには**交差検証法 (cross validation)** も選択肢としてあるため、こちらも必要であれば挑戦してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : val = 0.7 : 0.3 の割合で分割する\n",
    "train_x, val_x, train_t, val_t = train_test_split(x, t, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2730, 224, 224, 3), (2730,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# サイズの確認（訓練データ）\n",
    "train_x.shape, train_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1170, 224, 224, 3), (1170,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# サイズの確認（検証データ）\n",
    "val_x.shape, val_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. モデルを定義\n",
    "\n",
    "今回は非常にシンプルな構成の CNN でモデルを構築します。あまり良い精度が出ないようにしてあるため、ここのモデルの構造を工夫して精度を高めましょう。\n",
    "\n",
    "ヒント\n",
    "- VGG, ResNet, MobileNet\n",
    "- ファインチューニング\n",
    "\n",
    "複雑なモデルを構築する場合には GPU がないと遅い場合もあるため、[こちらの手順](https://www.kikagaku.ai/tutorial/guide_for_beginners/learn/platform_environment) を参考に Google Colab を使ってみると良いでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# バージョンの情報\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル構築の際に乱数のシードも固定しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def reset_seed(seed=0):\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    random.seed(seed) #　random関数のシードを固定\n",
    "    np.random.seed(seed) #numpyのシードを固定\n",
    "    tf.random.set_seed(seed) #tensorflowのシードを固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シードの固定を実行\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow のモデルの定義に必要なモジュールを読み込みます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようなモデルを定義していきましょう。\n",
    "\n",
    "- Original: (224, 224, 3)\n",
    "- => Convolution (Relu) => (224, 224, 6)\n",
    "- => Half Pooling => (112, 112, 6)\n",
    "- => ベクトル化 (112×112×6)\n",
    "- => 全結合層 (112×112×6 => 100), Relu\n",
    "- => 全結合層 (100 => 2), Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# モデルの定義\\nmodel = models.Sequential([\\n    # Convolution\\n    layers.Conv2D(3, kernel_size=(3,3), padding='same', activation='relu', input_shape=(224, 224, 3)),\\n    # Pooling\\n    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\\n    # ベクトル化 (Flatten)\\n    layers.Flatten(),\\n    # 全結合層\\n    layers.Dense(100, activation='relu'),\\n    # 全結合層\\n    layers.Dense(2, activation='softmax') \\n])\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# モデルの定義\n",
    "model = models.Sequential([\n",
    "    # Convolution\n",
    "    layers.Conv2D(3, kernel_size=(3,3), padding='same', activation='relu', input_shape=(224, 224, 3)),\n",
    "    # Pooling\n",
    "    layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    # ベクトル化 (Flatten)\n",
    "    layers.Flatten(),\n",
    "    # 全結合層\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    # 全結合層\n",
    "    layers.Dense(2, activation='softmax') \n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = models.Sequential()\n",
    "\n",
    "conv_base.add(layers.Conv2D(3, kernel_size=(3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "conv_base.add(layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenrsoflow では定義したモデルをコンパイルする必要がありました。最適化のアルゴリズムを確率的勾配降下法 (SGD) として選択します。\n",
    "\n",
    "ここのアルゴリズムの選択と学習係数の設定も精度向上のポイントですので、[こちらの記事](https://www.tensorflow.org/guide/keras/train_and_evaluate) などを参考にしながら進めてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# モデルのコンパイル\\nmodel.compile(\\n    loss='binary_crossentropy',\\n    optimizer=optimizers.RMSprop(lr=2e-5),\\n    metrics=['acc']\\n)\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11/11 [==============================] - 9s 824ms/step - loss: 1.9503 - accuracy: 0.5172 - val_loss: 0.8333 - val_accuracy: 0.5154\n",
      "Epoch 2/20\n",
      "11/11 [==============================] - 8s 756ms/step - loss: 0.9194 - accuracy: 0.5542 - val_loss: 0.6916 - val_accuracy: 0.6068\n",
      "Epoch 3/20\n",
      "11/11 [==============================] - 10s 870ms/step - loss: 0.6467 - accuracy: 0.6374 - val_loss: 0.6342 - val_accuracy: 0.6068\n",
      "Epoch 4/20\n",
      "11/11 [==============================] - 9s 807ms/step - loss: 0.5330 - accuracy: 0.7161 - val_loss: 0.5298 - val_accuracy: 0.7479\n",
      "Epoch 5/20\n",
      "11/11 [==============================] - 9s 795ms/step - loss: 0.4717 - accuracy: 0.7802 - val_loss: 0.5110 - val_accuracy: 0.7675\n",
      "Epoch 6/20\n",
      "11/11 [==============================] - 8s 759ms/step - loss: 0.4352 - accuracy: 0.8187 - val_loss: 0.5490 - val_accuracy: 0.6795\n",
      "Epoch 7/20\n",
      "11/11 [==============================] - 8s 686ms/step - loss: 0.4179 - accuracy: 0.8253 - val_loss: 0.4410 - val_accuracy: 0.8256\n",
      "Epoch 8/20\n",
      "11/11 [==============================] - 9s 785ms/step - loss: 0.3796 - accuracy: 0.8623 - val_loss: 0.4359 - val_accuracy: 0.8197\n",
      "Epoch 9/20\n",
      "11/11 [==============================] - 9s 794ms/step - loss: 0.3497 - accuracy: 0.8952 - val_loss: 0.4182 - val_accuracy: 0.8265\n",
      "Epoch 10/20\n",
      "11/11 [==============================] - 9s 783ms/step - loss: 0.3235 - accuracy: 0.9183 - val_loss: 0.4032 - val_accuracy: 0.8350\n",
      "Epoch 11/20\n",
      "11/11 [==============================] - 8s 712ms/step - loss: 0.3067 - accuracy: 0.9136 - val_loss: 0.3878 - val_accuracy: 0.8376\n",
      "Epoch 12/20\n",
      "11/11 [==============================] - 7s 660ms/step - loss: 0.2911 - accuracy: 0.9147 - val_loss: 0.3657 - val_accuracy: 0.8547\n",
      "Epoch 13/20\n",
      "11/11 [==============================] - 8s 736ms/step - loss: 0.2870 - accuracy: 0.9103 - val_loss: 0.3534 - val_accuracy: 0.8590\n",
      "Epoch 14/20\n",
      "11/11 [==============================] - 7s 663ms/step - loss: 0.2610 - accuracy: 0.9245 - val_loss: 0.3613 - val_accuracy: 0.8444\n",
      "Epoch 15/20\n",
      "11/11 [==============================] - 8s 719ms/step - loss: 0.2412 - accuracy: 0.9337 - val_loss: 0.3405 - val_accuracy: 0.8487\n",
      "Epoch 16/20\n",
      "11/11 [==============================] - 8s 715ms/step - loss: 0.2348 - accuracy: 0.9278 - val_loss: 0.3350 - val_accuracy: 0.8521\n",
      "Epoch 17/20\n",
      "11/11 [==============================] - 8s 754ms/step - loss: 0.2087 - accuracy: 0.9458 - val_loss: 0.3197 - val_accuracy: 0.8615\n",
      "Epoch 18/20\n",
      "11/11 [==============================] - 8s 744ms/step - loss: 0.1926 - accuracy: 0.9557 - val_loss: 0.3116 - val_accuracy: 0.8761\n",
      "Epoch 19/20\n",
      "11/11 [==============================] - 8s 766ms/step - loss: 0.1775 - accuracy: 0.9630 - val_loss: 0.3040 - val_accuracy: 0.8744\n",
      "Epoch 20/20\n",
      "11/11 [==============================] - 8s 698ms/step - loss: 0.1837 - accuracy: 0.9502 - val_loss: 0.2958 - val_accuracy: 0.8726\n"
     ]
    }
   ],
   "source": [
    "# モデルの学習\n",
    "history = model.fit(\n",
    "    train_x, train_t,\n",
    "    batch_size=256,\n",
    "    #epochs=2,\n",
    "    epochs=20,\n",
    "    validation_data=(val_x, val_t)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train に関して accuracy が 0.76, validation に関しては accuracy が 0.49 となっており、**過学習 (overfitting)** が生じていることがわかります。\n",
    "\n",
    "この辺りも踏まえて、予測の精度を上げられるように色々な手法に挑戦してみましょう。比較的簡単なところでは以下を調整してみると良いでしょう。\n",
    "\n",
    "- モデルの構成（VGG, ResNet, MobileNet など）\n",
    "- ファイチューニング\n",
    "- 最適化アルゴリズムの選択\n",
    "- 学習係数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファインチューニング\n",
    "\n",
    "model をconv_baseと全結合層に分けて、conv_baseだけを取り出す\n",
    "\n",
    "一回、model.add()でも大丈夫かを確認しよう→大丈夫だった\n",
    "\n",
    "compileを変えて、ファインチューニングのをやってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(loss='binary_crossentropy',\\n              optimizer=optimizers.RMSprop(lr=2e-5),\\n              metrics=['acc'])\\n\\nhistory = model.fit_generator(\\n      train_generator,\\n      steps_per_epoch=100,\\n      #epochs=30,\\n      epochs=1,\\n      validation_data=validation_generator,\\n      validation_steps=50,\\n      verbose=2)\\n      \""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      #epochs=30,\n",
    "      epochs=1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,\n",
    "      verbose=2)\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 3)       84        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 3)       0         \n",
      "=================================================================\n",
      "Total params: 84\n",
      "Trainable params: 0\n",
      "Non-trainable params: 84\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'conv2d':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.compile(loss='binary_crossentropy',\\n              optimizer=optimizers.RMSprop(lr=1e-5),\\n              metrics=['acc'])\\n\\nhistory = model.fit_generator(\\n      train_generator,\\n      steps_per_epoch=100,\\n      #epochs=100,\\n      epochs=1,\\n      validation_data=validation_generator,\\n      validation_steps=50)\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      #epochs=100,\n",
    "      epochs=1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# モデルのコンパイル\\nmodel.compile(\\n    loss='binary_crossentropy',\\n    optimizer=optimizers.RMSprop(lr=1e-5),\\n    metrics=['acc']\\n)\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのコンパイル\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 15s 1s/step - loss: 0.9965 - accuracy: 0.7476 - val_loss: 0.3303 - val_accuracy: 0.8624\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.2687 - accuracy: 0.8835 - val_loss: 0.3998 - val_accuracy: 0.8188\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1793 - accuracy: 0.9527 - val_loss: 0.3023 - val_accuracy: 0.8701\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1482 - accuracy: 0.9703 - val_loss: 0.2838 - val_accuracy: 0.8769\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.1265 - accuracy: 0.9769 - val_loss: 0.2721 - val_accuracy: 0.8846\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.1085 - accuracy: 0.9824 - val_loss: 0.2766 - val_accuracy: 0.8821\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0928 - accuracy: 0.9853 - val_loss: 0.2690 - val_accuracy: 0.8863\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.0792 - accuracy: 0.9883 - val_loss: 0.2698 - val_accuracy: 0.8940\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 14s 1s/step - loss: 0.0672 - accuracy: 0.9890 - val_loss: 0.2752 - val_accuracy: 0.8889\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 13s 1s/step - loss: 0.0562 - accuracy: 0.9916 - val_loss: 0.2861 - val_accuracy: 0.8863\n"
     ]
    }
   ],
   "source": [
    "# モデルの学習\n",
    "history = model.fit(\n",
    "    train_x, train_t,\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    #epochs=100,\n",
    "    #epochs=30,\n",
    "    validation_data=(val_x, val_t)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. モデルの保存\n",
    "\n",
    "推論にて使用するために学習済みモデルをファイルに出力しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDF5 という形式で保存（TensorFlowではこちらを用いるようです）\n",
    "model.save('dog_cat_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで作業フォルダの中に `dog_cat_cnn.h5` というファイルが出力できていれば成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ONNX形式で出力\n",
    "\n",
    "ONNX は TensorFlow や PyTorch 問わず、モデルの形式を標準化するプロジェクトです。  \n",
    "最近では、こちらの形式に統一しておくほうが汎用性が高くなっているため、推論サーバーでは ONNX 形式を使用します。\n",
    "\n",
    "onnxruntime では通常よりも速度が速いと言われています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "# !pip install onnxruntime   or   !python -m pip install onnxruntime\n",
    "\n",
    "# macOS\n",
    "#!pip3 install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "onnxruntime.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "# !pip install keras2onnx  or   !python -m pip install keras2onnx\n",
    "\n",
    "# macOS\n",
    "#!pip3 install keras2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import onnx\n",
    "import keras2onnx\n",
    "keras2onnx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to find out a correct type for tensor type = 20 of conv2d/Conv2D/ReadVariableOp/resource:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-10ea9bda513e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Keras -> ONNX の変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras2onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages/keras2onnx/main.py\u001b[0m in \u001b[0;36mconvert_keras\u001b[0;34m(model, name, doc_string, target_opset, channel_first_inputs, debug_mode, custom_op_conversions)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mparse_graph_modeless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopology\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_opset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mparse_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopology\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_opset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages/keras2onnx/parser.py\u001b[0m in \u001b[0;36mparse_graph\u001b[0;34m(topo, graph, target_opset, output_names, keras_node_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m     return _parse_graph_core_v2(\n\u001b[1;32m    840\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras_node_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m     \u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_keras\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_parse_graph_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         graph, keras_node_dict, topo, top_level, output_names)\n",
      "\u001b[0;32m~/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages/keras2onnx/parser.py\u001b[0m in \u001b[0;36m_parse_graph_core_v2\u001b[0;34m(graph, keras_node_dict, topology, top_scope, output_names)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0m_on_parsing_time_distributed_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlayer_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mget_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mon_parsing_keras_layer_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0m_on_parsing_tf_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodelist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages/keras2onnx/_parser_tf.py\u001b[0m in \u001b[0;36mon_parsing_keras_layer_v2\u001b[0;34m(graph, layer_info, varset, prefix)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0miname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mk2o_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\tinput : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mvar_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_input_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_variable_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvarset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_opset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mi0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvarset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_variable_or_declare_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/情報学館授業資料/s２/特論７/univ_tokyo_2020/tensorflow/venv/lib/python3.7/site-packages/keras2onnx/_parser_tf.py\u001b[0m in \u001b[0;36minfer_variable_type\u001b[0;34m(tensor, opset, inbound_node_shape)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         raise ValueError(\n\u001b[0;32m---> 48\u001b[0;31m             \"Unable to find out a correct type for tensor type = {} of {}\".format(tensor_type, tensor.name))\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to find out a correct type for tensor type = 20 of conv2d/Conv2D/ReadVariableOp/resource:0"
     ]
    }
   ],
   "source": [
    "# Keras -> ONNX の変換\n",
    "onnx_model = keras2onnx.convert_keras(model, model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存\n",
    "keras2onnx.save_model(onnx_model, 'dog_cat_cnn88.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "session = onnxruntime.InferenceSession('dog_cat_cnn.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論に必要な構造の抽出\n",
    "session.get_modelmeta()\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name, output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論\n",
    "y_probs = session.run([output_name], {input_name: x})[0]\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベル\n",
    "y = np.argmax(y_prob)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
